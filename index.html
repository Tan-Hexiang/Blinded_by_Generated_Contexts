<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  

  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?</title>
  <style>
    .image-container {
        text-align: center;
        margin-bottom: 20px;
    }
    .image-container img {
        width: 50%;
        height: 50%;
    }
    .image-caption {
        text-align: center;
        font-size: 1.25em;
        margin-top: 10px;
    }
</style>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts?</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://tan-hexiang.github.io/" target="_blank">Hexiang Tan</a>,</span>
                <span class="author-block">
                  <a href="http://ofey.me/" target="_blank">Fei Sun</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://yangwl.site/" target="_blank">Wanli Yang,</a></span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=v1KzwYEAAAAJ&hl=en" target="_blank">Yuanzhuo Wang,</a></span>
                  <span class="author-block">
                    <a href="https://caoqi92.github.io/biography/" target="_blank">Qi Cao, </a></span>
                  <span class="author-block">
                    <a href="https://people.ucas.ac.cn/~cxq?language=en" target="_blank">Xueqi Cheng,</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">CAS Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences<br>ACL 2024 Main</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2401.11911.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Tan-Hexiang/RetrieveOrGenerated" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2401.11911" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While auxiliary information has become a key to enhancing Large Language Models (LLMs), relatively little is known about how LLMs merge these contexts, specifically contexts generated by LLMs and those retrieved from external sources.
            To investigate this, we formulate a systematic framework to identify whether LLMs' responses are attributed to either generated or retrieved contexts.
            To easily trace the origin of the response, we construct datasets with conflicting contexts, i.e., each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer.
            Our experiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to favor generated contexts, even when they provide incorrect information.
            We further identify two key factors contributing to this bias: i) contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of being selected; ii) the segmentation process used in retrieved contexts disrupts their completeness, thereby hindering their full utilization in LLMs.
            Our analysis enhances the understanding of how LLMs merge diverse contexts, offers valuable insights for advancing current LLM augmentation methods, and highlights the risk of generated misinformation for retrieval-augmented LLMs.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="hero is-small">
  <div class="hero-body">
    <div class="columns is-centered has-text-centered">
    <div class="container">
       <h2 class="title is-3">Context-Conflicting Datasets</h2>
       <div class="content has-text-justified">
        <p>We construct context-conflicting datasets where only one of generated and retrieved contexts contain correct answer for the question.</p>
          <p>
            <ul>
              <li><strong>AIG:</strong> Answer In Generated Contexts</li>
              <li><strong>AIR:</strong> Answer In Retrieved Contexts</li>
            </ul>
          </p>
      </div>
      <div style="text-align: center;">
        <img src="static/images/datasets.png" alt="MY ALT TEXT"/>
      </div>
        <h3 class="subtitle has-text-centered">Figure 1. The framework of constructing context-conflicting datasets.</h3>
</div>
</div>
</div>
</section>
<!--  End image carousel -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
       <h2 class="title is-3">LLMs Prefer Generated Contexts</h2>
       <div class="content has-text-justified">
       <p>LLMs prefer generated contexts, even when they are <strong></strong>incorrect</strong>. This bias is consistent across various generator, reader and retriever models.
       </p>
      </div>
      <div class="image-container">
        <img src="static/images/bias.png" alt="MY ALT TEXT"/>
        <h3 class="image-caption">
          Figure 2: The extent of LLMs' bias towards generated contexts on NQ-AIR datasets where generated contexts are wrong.
        </h3>
    </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
       <h2 class="title is-3">Why LLMs Prefer Generated Contexts?</h2>
       <div class="content">
          <ul style="text-align: left;">
            <li><strong>Confirmation bias is not a key factor</strong>: LLMs maintain a significant preference for generated contexts when they contain information inconsistent with LLMs’ parametric knowledge.</li>
            <li><strong>Text similarity is a significant factor</strong>: compared to retrieved contexts, generated contexts typically exhibit a higher degree of similarity to the questions, even when they contain incorrect information. The samples with a larger similarity gap between generated and retrieved contexts exhibit a more pronounced bias.</li>
            <li><strong>Semantic completeness matters</strong>: LLMs tend to favor contexts with semantic integrity. The segmentation process used in retrieved contexts may disrupt their completeness, thereby hindering their full utilization in LLMs.</li>
          </ul>
        </div>
      <div class="image-container">
        <img src="static/images/similarity.png" alt="MY ALT TEXT"/>
        <h3 class="image-caption">
          Figure 3: Generated contexts typically exhibit a higher degree of similarity to the questions.
        </h3>
</div>
</div>
</section>



<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{tan2024blinded,
        title={Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?}, 
        author={Hexiang Tan and Fei Sun and Wanli Yang and Yuanzhuo Wang and Qi Cao and Xueqi Cheng},
        year={2024},
        eprint={2401.11911},
        archivePrefix={arXiv},
        primaryClass={cs.CL}
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
